{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa2bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: Attention Free Transformer.pdf\n",
      "  ✓ Loaded 29 pages\n",
      "\n",
      "Processing: Attention is All You Need Paper.pdf\n",
      "  ✓ Loaded 11 pages\n",
      "\n",
      "Processing: IJRAR Report.pdf\n",
      "  ✓ Loaded 12 pages\n",
      "\n",
      "Total documents loaded: 52\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e50fd5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='An Attention Free Transformer\\nShuangfei Zhai\\nApple Inc.\\nszhai@apple.com\\nWalter Talbott\\nApple Inc.\\nwtalbott@apple.com\\nNitish Srivastava\\nApple Inc.\\nnitish_srivastava@apple.com\\nChen Huang\\nApple Inc.\\nchen-huang@apple.com\\nHanlin Goh\\nApple Inc.\\nhanlin@apple.com\\nRuixiang Zhang ∗\\nApple Inc., MILA\\nruixiang_zhang2@apple.com\\nJosh Susskind\\nApple Inc.\\njsusskind@apple.com\\nAbstract\\nWe introduce Attention Free Transformer (AFT), an efﬁcient variant of Transform-\\ners [1] that eliminates the need for dot product self attention. In an AFT layer, the\\nkey and value are ﬁrst combined with a set of learned position biases, the result of\\nwhich is multiplied with the query in an element-wise fashion. This new operation\\nhas a memory complexity linear w.r.t. both the context size and the dimension\\nof features, making it compatible to both large input and model sizes. We also\\nintroduce AFT-local and AFT-conv, two model variants that take advantage of the\\nidea of locality and spatial weight sharing while maintaining global connectivity.\\nWe conduct extensive experiments on two autoregressive modeling tasks (CIFAR10\\nand Enwik8) as well as an image recognition task (ImageNet-1K classiﬁcation).\\nWe show that AFT demonstrates competitive performance on all the benchmarks,\\nwhile providing excellent efﬁciency at the same time.\\n1 Introduction\\nSelf attention mechanisms, represented by Transformers [1], have driven the advancement of various\\nmachine learning problems, including language understanding [2, 3] and computer vision applications\\n[4–6]. Different from classic model architectures such as Convolutional Neural Nets (CNNs) or\\nRecurrent Neural Nets (RNNs), Transformers enable direct interaction between every pair of elements\\nwithin a sequence, which makes them especially powerful at capturing long term dependencies.\\nHowever, Transformers require high computational costs. The cause of this challenge is the need to\\nperform attention operations that have quadratic time and space complexity w.r.t. the context size.\\nThis makes it difﬁcult for Transformers to scale to inputs with large context sizes. A number of recent\\nworks have been dedicated to addressing the scalability issue of Transformers [7–13]. The common\\nidea here is to approximate the full attention operation, with the techniques ranging from sparsity,\\nlocality sensitive hashing, low rank decomposition, kernel approximation, etc..\\nIn this paper, we propose a computational module that does not use or approximate the standard\\ndot product attention. We hence name our model the attention free transformer (AFT). Similar to\\ndot product attention, AFT is composed of the interaction of three quantities, namely the query, key\\nand value (Q,K,V ). The difference is that, in AFT the key and value (context) are ﬁrst combined\\n∗work done while interning at Apple.\\nPreprint. Under review.\\narXiv:2105.14103v2  [cs.LG]  21 Sep 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 1: Left: average relative 2d attention maps from a pretrained 12 layer 6 head ViT [5]. Right:\\nrelative position biases learned by a AFT-conv with comparable size. Each row represents a layer\\n(with layer index ranging from {0, 2, 4, 6, 8, 10}); Each column represents a head. See the Appendix\\nfor a more complete version.\\nTable 1: Complexity comparison with different Transformers: Reformer [ 8], Linear Transformer\\n[11], Performer [13] (only variants that support the causal mode are shown). Here T,d denote the\\nsequence length and feature dimension, respectively.\\nModel Time Space\\nTransformer O(T2d) O(T2 + Td)\\nReformer O(T log Td) O(T log T + Td)\\nLinear Transformer O(Td2) O(Td + d2)\\nPerformer O(Td2 log d) O(Td log d + d2 log d)\\nAFT-simple O(Td) O(Td)\\nAFT-full O(T2d) O(Td)\\nAFT-local (AFT-conv) O(Tsd), s < T O(Td)\\ntogether with a set of learned position biases. The query is then combined with the reduced context\\nwith element-wise multiplication. See Figure 2 for an illustration.\\nAFT maintains direct interaction between any two points in the context, which is a major advantage\\nof dot product attention. In fact, AFT can be interpreted as performing attention where the number of\\nattention heads is the same as the model’s feature dimension, whereas the attention maps do not need\\nto be explicitly computed (see Sec. 3.1 for details). This results in a memory complexity linear w.r.t.\\nboth the input and model sizes.\\nThe rearranged computational ordering of Q,K,V is also found in recent “linearized attention\"\\nworks [11, 13–15]. The difference is that AFT combines kand vin an element-wise fashion, while\\nall the linear attention papers rely on matrix dot products. The latter approach results in an complexity\\nquadratic to the model’s feature dimension, which is unfriendly to large model sizes. See Table 1 for\\nthe complexity analysis of AFT in comparison to other variants.\\nEmpirically, we observed that trained Transformers tend to demonstrate extensive local patterns (see\\nFig. 1). This motivates us to propose two variants of AFT: AFT-local and AFT-conv. In AFT-local,\\nthe learned position biases are constrained to a local region, while global connectivity is maintained.\\nAFT-conv further extends this design by imposing spatial weight sharing, effectively making it a\\nvariant of CNN with global receptive ﬁeld. We show that the locality constraint not only provides\\nbetter parameter and computational efﬁciency, but also greatly improves model’s performance in all\\ntasks.\\nWe perform experiments with AFT on image auto-regressive modeling, character level language\\nmodeling, and image classiﬁcation tasks. We show that AFT provides competitive performance, often\\nmatching or beating standard Transformers and other variants, while providing excellent efﬁciency.\\nWe also provide extensive ablation studies to several design choices of AFT, and discuss its unique\\nproperties such as compatibility with Transformers, sparsity and variable sized inputs.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='2 Multi-Head Attention\\nAt the core of Transformers is the Multi-Head Attention (MHA) operation. In the mode of self\\nattention, given an input sequence X ∈RT×d, and the number of heads h, MHA performs a scaled\\ndot product attention for each head i, deﬁned as:\\nfi(X) =σ(Qi(Ki)T\\n√dk\\n)Vi, s.t. Qi = XWQ\\ni ,Ki = XWK\\ni ,Vi = XWV\\ni , (1)\\nwhere WQ\\ni ∈Rd×dk , WK\\ni ∈Rd×dk , WV\\ni ∈Rd×dv are linear transformations for head i, and σis\\nthe non-linearity by default set as the softmax function (applied to each row of a matrix). dk,dv are\\ndimensions for key and value, respectively. MHA concatenates the output of hattention heads along\\nthe channel dimension, resulting in feature dimension hdv. Unless otherwise mentioned, we assume\\ndk = dv and h= d\\ndk\\n. This means the query, key and value are the same dimension within each head,\\nand the output dimension matches that of the input.\\n3 Methodology\\n3.1 Attention Free Transformer\\nWe now deﬁne Attention free transformer (AFT), which is a plugin replacement of MHA without the\\nneed of changing other architectural aspects of Transformers. Given the input X, AFT ﬁrst linearly\\ntransforms them into Q= XWQ, K = XWK, V = XWV, then performs following operation 2:\\nY = f(X); Yt = σq(Qt) ⊙\\n∑T\\nt′=1 exp(Kt′ + wt,t′ ) ⊙Vt′\\n∑T\\nt′=1 exp(Kt′ + wt,t′ )\\n(2)\\nwhere ⊙is the element-wise product; σq is the nonlinearity applied to the query with default being\\nsigmoid; w∈RT×T is the learned pair-wise position biases (see Figure 2 for an illustration).\\nExplained in words, for each target position t, AFT performs a weighted average of values, the result\\nof which is combined with the query with element-wise multiplication. In particular, the weighting\\nis simply composed of the keys and a set of learned pair-wise position biases. This provides the\\nimmediate advantage of not needing to compute and store the expensive attention matrix, while\\nmaintaining the global interactions between query and values as MHA does.\\nIn order to further see AFT’s relationship to MHA, we can rewrite Equation 2 as:\\nYi\\nt =<ai\\nt,V i >, s.t.ai\\nt = σq(Qi\\nt) exp(Ki + wt)∑T\\nt′=1 exp(Ki\\nt′ + wt,t′ )\\n, i= 1,2,...,d, t = 1,2,...,T. (3)\\nHere we use the superscript ito index the feature dimension of a matrix; <·,·>denotes the dot\\nproduct of vectors. In this rearranged form, we are able to express AFT in terms of attention again.\\nSpeciﬁcally, for each position, we have an attention vector ai\\nt ∈RT for each dimension, composed\\nof Q,K,w . In other words, AFT can be interpreted as performing implicit attention with as many\\nheads as feature dimensions, where the attention matrices take a factorized form.\\n3.2 AFT variants: locality, weight sharing and parameterization\\nAFT-full. We denote the basic version of AFT deﬁned in Equation 2 as AFT-full.\\nAFT-local. In many applications, locality is an important inductive bias, which has been exploited\\nby CNNs and recent works in Transformers [ 4, 7]. In addition, we found that trained standard\\nTransformers tend to demonstrate extensive local attention patterns. To be concrete, we visualized an\\nImagenetNet pretrained Vision Transformer (ViT) [5], which consists of 12 layers each with 6 heads.\\nFor the sake of visualization, we ignore the classiﬁcation tokens, and reshape each layer’s attention\\ntensors to shape 6 ×196 ×196 (the spatial size of the ViT’s feature maps is 14 ×14). We then\\nsampled 256 images from the ImageNet validation set. For each layer and each head, we compute the\\naverage relative 2d attentions, averaged across query positions and images. This results in a set of\\nattention maps of size 12 ×6 ×27 ×27 3. The result is shown in Figure 1 (left), where we show the\\n2we use the non-masked mode for illustration, and the masked/causal mode can be constructed by limiting\\nthe range of the summation.\\n312 is #layers, 6 is #heads, 27 × 27 is relative 2d attention size from feature map 14 × 14\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='∑T\\nt′ =1\\n∑T\\nt′ =1\\n⊙\\nQt exp\\nexp\\nK V\\nK\\n=\\n][ ⊙\\n)(\\n()(σq\\nwt\\n+\\n)\\nwt\\n+ Yt\\nFigure 2: An illustration of AFT deﬁned in Equation 2, with T = 3,d = 2.\\nattentions for every 2 layers (see the Appendix for the full visualization). We see that the relative\\nattention maps demonstrate strong local patterns (as indicated by the sharpness), especially in the\\nlower layers. This motivates a variant of AFT, dubbed AFT-local, where we only apply a learned set\\nof relative position biases locally:\\nwt,t′ =\\n{wt,t′ , if |t−t′|<s\\n0, otherwise. (4)\\nHere s≤T is a local window size. AFT-local provides further computational savings, both wrt the\\nnumber of parameters and time/space complexity. Note that different from local Transformers (e.g.,\\n[7]), AFT-local maintains global connectivity regardless of the window size s. In the experiments\\nwe verify the effectiveness of this design choice.\\nAFT-simple. An extreme form of AFT-local is when s= 0, i.e., no position bias is learned. This\\ngives rise to an extremely simple version of AFT, where we have:\\nYt = σq(Qt) ⊙\\n∑T\\nt′=1 exp(Kt′ ) ⊙Vt′\\n∑T\\nt′=1 exp(Kt′ )\\n= σq(Qt) ⊙\\nT∑\\nt′=1\\n(softmax(K) ⊙V)t′ . (5)\\nIn this version, the context reduction is further simpliﬁed to element-wise operations and global\\npooling. AFT-simple is similar to linearized attention [ 11, 13, 14], which is formulated as Yt =\\nφ(Qt) ∑T\\nt′=1\\n(\\nφ(Kt′ )T Vt′\\n)\\nφ(Qt) ∑T\\nt′=1 φ(Kt)T . However, it is easy to see that AFT-simple completely gets rid of the need\\nfor dot products operations, which results in a complexity of O(Td) rather than O(Td2).\\nAFT-conv.We can also further extend the idea of locality to incorporate spatial weight sharing, i.e.,\\nconvolution. This variant is especially relevant to vision tasks, as it is often desirable to extend a\\npretrained model to variable sized inputs. Speciﬁcally, we let the value of wt,t′ to be dependent only\\non the relative positions of tand t′, w.r.t. to a given spatial grid (1d or 2d). Similar to CNNs, we can\\nalso learn multiple sets of position biases (we reuse the notion of heads for reference). To account for\\nthe growth of #parameters as #heads increases, we adopt a design choice to tie the dimensionality\\nof K with #heads. This makes AFT-conv amendable to an implementation relying on depth-wise\\nseparable convolutions, global pooling and element-wise operations.\\nWe now show an example of AFT-conv with 1d inputs, 2d and 3d inputs can be derived similarly. We\\ndenote a model conﬁguration as AFT-conv-h-s, where h is the number of heads and sis the 1d local\\nwindow size. We now have w∈Rh×s, Q,V ∈RT×h×d\\nh , K ∈RT×h. For each head i= 1,2,...,h ,\\nwe have:\\nYi\\nt = σq(Qi\\nt) ⊙conv1d(exp(Ki) ⊙Vi, exp(wi) −1) +∑T\\nt′=1 exp(Ki\\nt′ ) ⊙Vi\\nt′\\nconv1d(exp(Ki), exp(wi) −1) +∑T\\nt′=1 exp(Ki\\nt′ )\\n. (6)\\nHere Yi\\nt ∈R\\nd\\nh , Qi,V i ∈RT×d\\nh , Ki ∈RT, wi ∈Rs; conv1d(x,w) is depth-wise separable\\n1d convolution operation where the convolutional ﬁlter w is shared across channel dimension 4.\\n4Equation 6 can also be implemented with fully connected operations, e.g., einsum, which might yield better\\nefﬁciency in practice.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Note that Equation 6 can be readily interpreted as a specialized convolutional layer with 1) global\\nconnectivity, 2) non-negative convolutional weights and 3) sophisticated divisive/multiplicative\\ngating mechanism. We show experimentally that all of the three aspects contribute signiﬁcantly to\\nAFT-conv’s performance.\\nParameterization. Empirically, we ﬁnd that it is important to parameterize the position biases w\\nproperly. For AFT-full and AFT-local, we adopt a factorized form ofwas:\\nwt,t′ = uT\\nt v′\\nt, u∈RT×d′\\n,v ∈RT×d′\\n, (7)\\nwhere d′is a small embedding dimension (e.g., 128). This simple factorization not only greatly\\nreduces the parameter counts (2Td′vs T2), but also empirically improves model’s performance in\\nboth training and testing.\\nFor AFT-conv, the factorization trick is non-applicable. We instead adopt a simple re-parameterization,\\nwhere for each head i, we let\\nwi = γiwi −mean(wi)\\nstd(wi) + βi, (8)\\nwhere γ ∈Rh,β ∈Rh are learnable gain and bias parameters, both initialized as 0.\\n4 Related Work\\nSince the Transformer was introduced, there have been numerous attempts to address the major\\nsource of inefﬁciency in the architecture, the quadratic cost of the attention operation. Improving this\\noperation can enable larger context sizes and more efﬁcient implementations. For a comprehensive,\\nrecent survey of efﬁcient transformers, see [16].\\nApproximating the dot product. [11, 13, 14] propose to approximate the exponential kernel with\\ninner product of projections, which leads to a linearized attention operation of complexity O(Td2).\\nThe d2 term of these models however makes it difﬁcult to scale with model size, which is not a\\nproblem for AFT. Reformers [8] apply LSH as an approximation to the dot product, where AFT\\ncompletely gets rid of it.\\nSparse, local attention. Sparse Transformers [7] and Image Transformer [17] proposes to use ﬁxed\\nsparse or local context patterns. Attention models in vision tasks (often combined with convolutions)\\nuse image structure to help handcraft relevant spatial patterns to attend [ 18–22]. AFT-local also\\nborrows the locality idea, but we put it as a bias rather than hard constraint. This allows AFT-\\nlocal/AFT-conv to take advantage of the full context, rather than relying only on a subset.\\nContext compression. Other approaches try to learn context patterns. Adaptive-Span Transformers\\n[23] learn a range for each attention head within which to attend. Routing transformers [ 24] use\\nclustering to compute dot-product attention only over a subset of elements within the same cluster.\\nThe Linformer [10] reduces the length of the context by compressing the keys and values with a\\nlinear layer. Compressive Transformers [9] compute and update reduced representations of the input\\nthat are far enough back in the input sequence, and attend to those compressed representations. AFT\\nis largely complementary to these approaches, as our focus is to improve the complexity of any given\\nsequence from the operation level.\\nEliminating dot product attention. Instead of limiting the number of comparisons, other methods\\nchange the operation used to compute attention. The Synthesizer [12] uses attention weights predicted\\nfrom inputs, rather than derived from dot-product interactions. The LightConv module introduced\\nin [ 25] proposes to replace the dot product self-attention with dynamic lightweight depthwise\\nconvolution, where the weights are normalized across temporal dimension. The Sinkhorn Transformer\\n[26] uses a differentiable sorting operation to identify relevant comparisons that may not be local in\\nthe original sequence order. AFT offers a different approach along this line, while highlighting strong\\nempirical performance and efﬁciency.\\nMLPs for vision. Concurrent works [ 27, 28] explore the use of MLP inplace of the attention\\noperation for vision tasks. While AFT can be viewed in a similar way, it is also equipped with a more\\nsophisticated gating mechanism. In particular, the weighting of values are composed of both the key\\nand position biases, which are normalized to non-negative values (similar to attention). This allows\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Table 2: NLL results on CIFAR10, evaluated by bits/dim, the lower the better. Speed and memory\\nare measured during training time, with a batch size of 32 across 8 V100 GPUs. AFT achieve the\\nstate-of-the-art result in this setting, with signiﬁcant improvements wrt speed and memory over\\nstandard Transformer, Sparse Transformer [7] and Image Transformer [17].\\nMethod L d h Train loss Test loss Iters/Sec GB/GPU\\nPixelCNN - - - 3.08 3.14\\nPixelCNN++ - - - - 2.92\\nPixelSNAIL - - - - 2.85\\nSparse Transformer strided 128 256 2 - 2.80\\nImage Transformer local2d 12 512 4 - 2.90 1.61 22.3\\nTransformer 12 512 4 2.90 2.88 1.35 30.6\\nTransformer 24 256 2 2.90 2.86 1.36 30.4\\nAFT-local-256 12 512 1 2.78 2.80 1.68 11.4\\nAFT-local-256 24 256 1 2.75 2.74 1.67 12.8\\nAFT-simple 24 256 1 2.82 2.89 2.15 9.5\\nTable 3: The effect of factorized parameterization of the position bias, evaluated by autoregressive\\nmodeling on CIFAR10.\\n#params/layer Train loss Test loss\\nNon Factorized 9.6M 2.82 2.84\\nFactorized (default) 0.6M 2.75 2.74\\nAFT to be a plugin module to existing Transformers without any architectural changes and extra\\ntuning. Besides, AFT-conv inherits the valuable properties of CNNs, allowing it to achieve excellent\\nparameter efﬁciency, strong performance as well as ability to handle variable sized inputs.\\n5 Experiments\\nWe conduct experiments on three tasks: image autoregressive modeling (Sec. 5.1), character level\\nlanguage modeling (Sec. 5.2) and image classiﬁcation (Sec. 5.3). The ﬁrst two benchmarks use the\\ncausal model (or decoder model) of AFT, while the last one uses the encoding model. All the experi-\\nments are designed in the plug and play fashion, where we obtain a baseline Transformer architecture\\nfor the speciﬁc task and replace the attention module with an AFT module. Hyperparameters such as\\ninitialization, learning rate scheduling are also directly inherited from the Transformer counterparts.\\nUnless otherwise mentioned, all experiments are conducted on 8×V100 GPU machines.\\n5.1 Image Autoregressive Modeling\\nIn our ﬁrst set of experiments, we consider the problem of image autoregressive modeling by\\nminimizing the negative log likelihood (NLL). Similar to [ 17], we represent an RGB image as a\\nsequence of length H×W ×3, with H,W being the height and width, respectively. Each sub-pixel\\nis represented as a 256-way discrete variable. We use CIFAR10 as the benchmarking dataset.\\nOur reference Transformer design largely follows that of [4], where a transformer block consists of\\nan attention layer (AFT layer in our case) with residual connection and a 2 layer MLP with residual\\nconnections (with the feedforward dimension multiplier set to 4). Layer Normalization (LN) [29] is\\napplied in a “pre-act\" fashion. We adopt learned position embeddings, and use a set of shared token\\nembeddings and prediction heads across RGB. We use AFT-local with the factorized parameterization\\nfor this experiment. The hidden dimension for the factorization is 64, with u,v initialized with\\nN(0,10−2); the local (1d) window size sis 256.\\nWe use AdamW [30], and follow a standard warmup learning rate schedule as in [ 1]. We use an\\ninitial learning rate of 3 ×10−3 a weight decay of 0.1 applied to all linear transformations weights,\\nand a dropout rate of 0.1. We adopt simple data augmentation. During training, we ﬁrst randomly ﬂip\\neach image horizontally, then add or subtract a value in the range [−10,10] from all its subpixels,\\nand clip resulting pixel values to [0,255]. We use cross entropy loss, and a default batch size of 128\\nfor 200 training epochs.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Table 4: Enwik8 results, measured in bits per character (bpc), the lower the better. Baselines compared\\nare Reformer [ 8], Synthesizer [ 12] (its best performing dense version), Linear Transformer [ 11]\\nand Performer [13]. L, d, h, T denote number of blocks (depth), dimension of features, number of\\nheads, and sequence length, respectively. Speed and memory are measured during training time,\\nwith a batch size of 128 on a 8 V100 GPU node. Both Linear Transformer and Performer are\\nimplemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other\\nmodels are implemented in native Pytorch.\\nMethod L d h T Train bpc Test bpc Iters/Sec GB/GPU\\nTransformer 12 512 8 1024 0.977 1.137 1.42 29.4\\nTransformer 24 256 4 1024 1.039 1.130 1.57 28.3\\nReformer 12 512 8 1024 1.04 1.195 1.05 20.9\\nSynthesizer 12 512 8 1024 0.994 1.298 1.49 29.9\\nLinear Transformer 12 512 8 1024 0.981 1.207 1.46 10.6\\nPerformer 12 512 8 1024 1.002 1.199 1.44 10.1\\nAFT-local-32 12 512 1 1024 0.854 1.180 1.85 11.3\\nAFT-local-32 24 256 1 1024 0.972 1.154 2.04 11.2\\nAFT-simple 24 256 1 1024 1.046 1.209 2.61 9.6\\nComparing with the state of the art. CIFAR10 is a crowded benchmark for image autoregressive\\nmodeling, and we compare with a few competitive baselines, as shown in Table 2. Note that CIFAR10\\nhas an unrolled sequence length of 3072, which is already prohibitive to train a full Transformer with\\nreasonable size. For the standard Transformer model, we adopt two conﬁgurations (L=12, d=512,\\nh=4 and L=24, d=256, h=2), with batch size 32 which is the largest one we can ﬁt on a 8xV100 GPU\\nnode. Another baseline is Image Transformer [17], which restricts attention to local2d windows of\\nsize of 256. We also compare to Sparse Transformers [7], which restrains attention to pre-speciﬁed\\nsparse subsets of context elements.\\nFrom Table2, we see that AFT-local outperforms all the Transformer baselines. We also observe that\\nthe deeper but narrower architecture is more effective than the shallow but wide baseline. Our best\\nmodel also achieves the state-of-the-art result on CIFAR10 in this setting, outperforming a much\\nlarger Sparse Transformer model. Efﬁciency wise, we benchmarked the Transformer variants against\\nAFT on a 8 V100 GPU node 5. All our variants are faster than standard Transformer and Image\\nTransformer, while consuming only half of the memory 6. Perhaps surprisingly, AFT-simple also\\nachieves very competitive performance, even outperforming the Image Transformer, while offering\\nexcellent speed and memory efﬁciency.\\nThe effect of factorization. We also provide ablations on the role of the factorized parameterization\\nof AFT. To do this, we retrained the best performing model from Table 2 ( i.e., AFT-local-256, L=24,\\nd=256) with a naively parameterized w, initialized with N(0,10−2). From Table 3, we see that the\\nfactorized version not only provides signiﬁcant parameter savings, but also improves the model’s\\nperformance both on training and testing.\\nTable 5: Training and testing bpc w.r.t. the local window size for AFT-local.\\nWin size 0 1 2 4 8 32 64 128 256 512 1024\\nTrain bpc 1.046 1.043 1.009 0.990 0.983 0.972 0.981 0.985 0.986 0.988 0.991\\nTest bpc 1.209 1.205 1.176 1.165 1.162 1.154 1.160 1.165 1.164 1.171 1.173\\n5We use a batch size of 32 which is the largest batch size Image Transformer can ﬁt\\n6Fair speed/memory comparison against Sparse Transformer is infeasible, as it relies on a set of advanced\\nimplementation tricks such as mixed precision and gradient checkpointing, whereas AFT is implemented with\\nstandard Pytorch utilities ran in full precision.\\nTable 6: Increasing T on Enwik8. Both training and testing loss are improved as T increases.\\nT 1024 2048 4096\\nTrain bpc 0.972 0.951 0.945\\nTest bpc 1.154 1.135 1.134\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='5.2 Language Modeling\\nWe apply AFT to character level language modeling on Enwik8 [ 31], which is another popular\\nbenchmark for auto-regressive modeling. We follow the standard preprocessing procedures and\\ntraining/validation/test splits as in [32]. Our base Transformer reference is a 12 layer 512 dimensional\\n8 head architecture with 2048 feed forward dimensions. For the ﬁrst set of experiments, we use\\nsequence length of 1024. Our training protocol is largely the same as the previous experiment, except\\nthat we increase the weight decay to 0.5 and train for 100 epochs with batch size 128. We evaluate the\\nAFT-local with a window size of 32 and d′= 256. We also compare to several efﬁcient Transformer\\nbaselines, namely Reformer [ 8], Synthesizer [ 12] , Linear Transformer [ 11] and Performer [ 13].\\nFrom Table 4, we see that with the base L = 12,d = 512architecture, AFT achieves the lowest\\ntraining bits per character (bpc), which is an indicator for high model capacity. Its test performance is\\nslightly worse than that of the basic Transformer, but outperforms all other Transformer variants. The\\ndeeper and narrower architecture of AFT strikes the best balance across parameter, speed, memory\\nand performance. Its test bpc is only 0.024 away from the full Transformer’s, while only consuming\\na third of the memory and provides a 44% speedup. AFT-simple again demonstrates competitive\\nperformance and excellent efﬁciency.\\nOn the local window size. In order to validate the effect of local window size, we performed\\nadditional experiments with the L = 24,d = 256architecture, ﬁxing everything but varying the\\nlocal window size s. We show the results in Table 5, where we see that both the training and testing\\nbpc forms a U-shape w.r.t. the window size, with 32 achieving the best performance. This further\\nconﬁrms that locality is indeed an effective inductive bias across tasks.\\nLonger sequence size. We are also interested in AFT’s ability to adapt to longer sequence sizes. Due\\nto its simplicity, one might even expect a degradation of performance as T increases. To this end,\\nwe trained the AFT-local-32, L=24, d=256 model with T increased to 2048 and 4096. The results\\nare shown in Table 6. We see that AFT is able to take advantage of larger sequence sizes and yield\\nconsistently lower training and testing loss as T increases.\\n5.3 Image Classiﬁcation\\nWe then test the non-causal version of AFT, focusing on an image classiﬁcation task. We adopt\\nthe Vision Transformer architecture [5], and perform experiments on the Imagent 1K classiﬁcation\\ndataset. We adopt training setting and hyper parameters (batch size, data augmentation, regularization\\nand learning rate scheduling) from DeiT [6].\\nIn a nutshell, A ViT splits an image into 16 ×16 non-overlapping patches, then linearly projects\\neach patch with shared weights to the equivalence of token embeddings. A learned class token is\\nappended to the resulting representation, resulting in a sequence of length T = 1 +H/16\\nW/16 . A linear\\nclassiﬁcation head is attached to the ﬁnal layer’s class token to obtain the ﬁnal outputs. See [5] for\\nmore details of the model conﬁguration. All the experiments are conducted on the ImageNet-1K\\ndataset, without using extra data.\\nSince the sequence size is relatively small in this task (T = 197for input sizes of 224 ×224), we\\nﬁrst experiment with AFT-full. The hidden dimension of factorized position bias is set as d′= 128.\\nBesides, we also experiment with AFT-conv. In this setting, we also remove the use of position\\nembedding and class token, and apply global average pooling after the ﬁnal layer’s output, which is\\nthen fed into the classiﬁcation linear layer. This modiﬁcation not only simpliﬁes the model design,\\nbut also makes AFT-convfully convolutional, which is absent from Transformer and its variants.\\nWe compare against two baseline Transformer conﬁgurations, with the “tiny\" (L=12, d=192, h=3)\\nand “small\" (L=12, d=384, h=6) conﬁgurations, respectively. We also consider Lambda Networks\\n[15], which is closely related to the linearized attention line of work. Similar to AFT-conv, we\\nremove the class token and apply global average pooling instead. We use a publicly available\\nimplementation 7, and apply the full context mode with the key projection dimension |k|= 16\\n(this setting invokes the faster linear implementation). We also apply BatchNorm to the query, key\\nprojections as recommended by [15].\\n7github.com/lucidrains/lambda-networks, released under MIT License\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Table 7: Imagenet 1K classiﬁcation results with the Transformer architecture from DeiT [6], cropsize\\nis 224. Speed and memory consumption are measured in inference mode on a V100 GPU, batch size\\nis 256.\\nModel Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)\\nResNet50 [33] 3 - 76.9 25.6 1257 6.5\\nDeiT tiny [6] - 3 72.2 5.7 2507 1.9\\nDeiT small [6] - 6 79.9 22.1 1010 2.9\\nLambda tiny [15] - 3 72.4 4.8 2157 2.7\\nLambda small [15] - 6 80.0 17.7 1057 5.8\\nAFT-full tiny - 1 72.4 6.3 2523 1.8\\nAFT-full small - 1 79.8 22.6 1011 2.6\\nAFT-conv tiny 11 32 73.9 5.4 2359 1.8\\nAFT-conv tiny 11 192 74.8 5.9 2365 2.2\\nAFT-conv small 11 16 80.2 20.3 989 2.5\\nAFT-conv small 11 384 80.8 22.5 936 3.2\\nAFT-conv small 15 384 81.0 23.0 936 3.2\\nOur result is shown in Table 7. We ﬁrst see that AFT-full achieves comparable performance with the\\nbaseline Transformer DeiT in both conﬁgurations, while with better memory footprint and similar\\nspeed. AFT-conv signiﬁcantly improves the top-1 accuracy of both conﬁgurations (2.%6, 1.1%\\nabsolute improvement for “tiny\" and “small\", respectively), with similar or smaller parameter counts.\\nCompared to Lambda Networks, all AFT variants achieve comparable or better accuracy, with\\ncomparable speed and much smaller memory footprints.\\nVisualization. We also tried to visualize the position biases (exp(w) −1 to be precise) learned by\\nAFT-conv, as shown in Figure 1 (right). Note that interesting local, symmetric sparse patterns emerge.\\nWe show in the Appendix that we can regularize the position biases to achieve more sparsity. We\\nalso show an extreme version of AFT-conv, where each head is assigned one non-zero context points,\\nwhile still keep good accuracy. This effectively transforms convolution into indexing.\\nVariable size inputs. AFT-conv is fully convolutional, which means that it can handle an input size\\ndifferent from that in training. We tested an AFT-conv model (last row of Table 7, trained with crop\\nsize 224) on a larger crop size of 384. This results in an improved accuracy of 81.6, compared with\\nthe original 81.0. This makes AFT-conv well suited for the pretraining ﬁnetuning workﬂows, as often\\nseen in Vision tasks.\\nCompatibility with Transformers. Although AFT is not designed to directly approximate MHA,\\nthey do share considerable similarity in that the value vectors are aggregated with learned non-\\nnegative weighting in both models. We hypothesize that representations learned by one model can be\\ntransferred to another. To test this, we obtain a pretrained “DeiT base\" model with crop size 384. We\\nthen train an AFT-conv by initializing its weights with that of the DeiT model, excluding the position\\nembeddings, the class token, key and query projections. We use a batch size of 64 and train the model\\nfor 100 epochs. As a control, we also train a randomly initialized AFT-conv for the same number of\\nepochs. The results are shown in Table 8. Interestingly, we see that the ﬁnetuned version of AFT-conv\\nachieves signiﬁcantly higher accuracy than that randomly initialized version. The resulting model is\\nalso more accurate, faster and memory efﬁcient than the original DeiT model.\\nGlobal connectivity. AFT-conv (as well as AFT-local) maintains global connectivity regardless\\nof the local kernel size, which is distinctive from sparse and local attention works. To see the\\nbeneﬁt of this design, we trained a degenerate variant of AFT-conv, where we modify Equation 4\\nto assign −∞values to wt,t′ outside the local window (zero weights after exponentiation). When\\nevaluating this baseline with kernel size 7, it gives a Top 1 accuracy of 79.9, compared to the default\\nAFT-conv’s 80.8 with the same setting, which is a 0.9% drop (we observe the same trend consistently\\nin various conﬁgurations). We hypothesize that this technique can also be extended to local and\\nsparse Transformers, but will leave it as future work.\\n6 Conclusions\\nWe have introduced the Attention Free Transformer that replaces dot product attention with an\\nefﬁcient new operation. We have demonstrated strong results on a set of standard benchmarks with\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Table 8: Finetuning AFT-conv for 100 epochs from a pretrained “DeiT base\" on 384 ×384 crops.\\n“ft\" and “rand\" stand for ﬁnetuning and random initialization, respectively.\\nModel Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)\\nDeit base [33] - 12 82.9 86.9 89.6 13.6\\nAFT-conv ft 25 32 83.4 79.7 98.5 8.9\\nAFT-conv rand 25 32 81.6 79.7 98.5 8.9\\nexcellent efﬁciency. We believe that our model opens a new design space for Transformer-like models,\\nand will see impact in various areas where self attention are needed.\\nReferences\\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\\nprocessing systems, pages 5998–6008, 2017.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n[3] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training.\\n[4] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining\\nfrom pixels.\\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929, 2020.\\n[6] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\\npreprint arXiv:2012.12877, 2020.\\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. CoRR, abs/1904.10509, 2019.\\n[8] Nikita Kitaev, L. Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. ArXiv,\\nabs/2001.04451, 2020.\\n[9] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and T. Lillicrap. Compressive trans-\\nformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2020.\\n[10] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\\nwith linear complexity. ArXiv, abs/2006.04768, 2020.\\n[11] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive\\ntransformers with linear attention. In Proceedings of the International Conference on Machine\\nLearning (ICML), 2020.\\n[12] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:\\nRethinking self-attention in transformer models, 2020.\\n[13] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger,\\nLucy Colwell, and Adrian Weller. Rethinking attention with performers, 2020.\\n[14] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.\\nRandom feature attention. In International Conference on Learning Representations, 2021.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='[15] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. In Interna-\\ntional Conference on Learning Representations, 2021.\\n[16] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey,\\n2020.\\n[17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku,\\nand Dustin Tran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\\n[18] Huiyu Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and Liang-Chieh Chen. Axial-deeplab:\\nStand-alone axial-attention for panoptic segmentation. ArXiv, abs/2003.07853, 2020.\\n[19] Zilong Huang, Xinggang Wang, Lichao Huang, C. Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\\nCriss-cross attention for semantic segmentation. 2019 IEEE/CVF International Conference on\\nComputer Vision (ICCV), pages 603–612, 2019.\\n[20] Zhen Zhu, Mengdu Xu, Song Bai, Tengteng Huang, and X. Bai. Asymmetric non-local neural\\nnetworks for semantic segmentation. 2019 IEEE/CVF International Conference on Computer\\nVision (ICCV), pages 593–602, 2019.\\n[21] Lang Huang, Y . Yuan, Jianyuan Guo, Chao Zhang, X. Chen, and Jingdong Wang. Interlaced\\nsparse self-attention for semantic segmentation. ArXiv, abs/1907.12273, 2019.\\n[22] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, I. Bello, Anselm Levskaya, and Jonathon\\nShlens. Stand-alone self-attention in vision models. ArXiv, abs/1906.05909, 2019.\\n[23] Sainbayar Sukhbaatar, E. Grave, P. Bojanowski, and Armand Joulin. Adaptive attention span in\\ntransformers. In ACL, 2019.\\n[24] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\\nattention with routing transformers. ArXiv, abs/2003.05997, 2020.\\n[25] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and M. Auli. Pay less attention with\\nlightweight and dynamic convolutions. ArXiv, abs/1901.10430, 2019.\\n[26] Yi Tay, Dara Bahri, L. Yang, Donald Metzler, and D. Juan. Sparse sinkhorn attention.ArXiv,\\nabs/2002.11296, 2020.\\n[27] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas\\nUnterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and\\nAlexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.\\n[28] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V . Le. Pay attention to mlps, 2021.\\n[29] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\n[31] Matt Mahoney. Large text compression benchmark, 2011.\\n[32] Zihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V . Le, and R. Salakhutdi-\\nnov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. ArXiv,\\nabs/1901.02860, 2019.\\n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition, 2015.\\n[34] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax,\\n2017.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 3: Exponentiated position biases learned by AFT-full, trained on ImageNet-1K, shown from\\nlayer 1, 2, ..., 12, arranged from top left to bottom right. Each image is of size 197 ×197, where\\nthe ﬁrst element corresponds to the class token, and the remaining 196 correspond to the 14 ×14\\npositions. We see that local, sparse patterns are learned without explicit supervision.\\nTable 9: The effect of factorized parameterization\\nof AFT-full.\\nTrain loss Top 1 Acc\\nNon Factorized 3.17 78.2\\nFactorized (default) 3.08 79.8\\nTable 10: The effect of reprameterization of AFT-\\nconv (kernel size 7 ×7).\\nTrain loss Top 1 Acc\\nNaive param 3.11 79.4\\nReparameterized (default) 2.94 80.8\\n7 Additional Ablations\\nWe conducted more experiments on the ImageNet-1K classiﬁcation settings.\\nFactorization of w. We ﬁrst verify the importance of the factorized parameterization of AFT-full.\\nAs shown in Tab 9, the non factorized parameterization of AFT-full achieves worse training and test\\nperformance than the factorized version.\\nReparameterization of w. For AFT-conv, we by default apply the reprameterization as described in\\nSec. 3.2. We verify that this design effectively improves the model’s performance, as shown in Table\\n10.\\nKernel size. We also experimented with varying the local window size based on AFT-conv small\\n(384 heads). The results are shown in Tab 11. Note that AFT-conv achieves comparable performance\\nto the Deit reference even with a very small kernel size of 3 ×3.\\nTable 11: Varying kernel size for AFT-conv.\\nKernel 3 7 11 15 25 27 DeiT small\\nTrain loss 3.02 2.94 2.94 2.93 2.93 2.94 3.01\\nTop 1 Acc 79.9 80.8 80.8 81.0 80.7 81.0 79.9\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 4: Image completion with the AFT-local trained on CIFAR10 autoregressive modeling task.\\nTop: masked images from the test set. Bottom: completed images.\\nTable 12: Top 1 accuracy of AFT-conv without the query term (w/o q). This results in signiﬁcant\\nperformance drops.\\nKernel 11 15\\nwith q (default) 80.8 81.0\\nw/o q 79.3 79.5\\nContribution of the query. The query term contributes a small fraction to the computation of AFT,\\nbut it contributes signiﬁcantly to AFT’s performance. We conducted an additional experiment with\\nAFT-conv (384 heads, kernel size in11 ×11 and 15 ×15), where we remove the query term. The\\nresult is shown in Tab 12.\\nVisualizing the key. The keys play a central role in AFT, as they provide content dependent\\nreweighting for effective context reduction. In order to understand their behavior, we visualized\\nthe feature maps for a AFT-conv model on randomly sampled images from the validation set of\\nImageNet-1K, as shown in Fig. 9, 10, 11, 12. Interestingly, we see that the keys gradually evolve to\\n“object detectors\" as the layer level goes up.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 5: The full set of average relative 2d attention maps learned by a pretrained ViT model (with 12\\nlayers and 6 heads) on ImageNet-1K. Each row corresponds to a layer and each column corresponds\\nto a head. Each attention map is of size 27 ×27, with the class token excluded.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 6: Exponentiated position biases learned by AFT-conv, trained on ImageNet-1K. Each row\\ncorresponds to a layer, each column corresponds to a head (the ﬁrst 16 are shown). This model has\\ntop 1 accuracy of 80.8%.\\nFigure 7: Exponentiated position biases learned by AFT-conv (kernel size 11 ×11) with sparsity\\nregularization, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds\\nto a head (the ﬁrst 16 are shown). This model has top 1 accuracy of 80.9%.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 8: Exponentiated position biases learned AFT-conv (kernel size11×11) with Gumbel softmax\\nsampling, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a\\nhead (the ﬁrst 16 are shown). This model has top 1 accuracy of 79.9%.\\n8 Sparsity\\nThe position biases learned by AFT-conv (kernel size11 ×11) as shown in Figure 6 demonstrates\\ninteresting sparsity patterns, which suggests great potential for quantization and pruning. To this end,\\nwe experimented with a simple sparsity promoting regularization term:\\nreg(w) =\\nh∑\\ni=1\\nH(wi), H(wi) =entropy(softmax(wi)). (9)\\nWhere we simply minimize the entropy for each head, with the softmax distribution using wi as\\nthe logits. We combining reg(w) with the cross entropy loss with a small weighting ( 0.001) and\\ntrain with the AFT-conv with kernel size 11 and 384 heads. This results in a slight improvement\\nin accuracy (due to its regularization effect) of 80.9 vs 80.8, as well as sparser looking position\\nbiases. The visualization is shown in Fig. 7. We see that the position biases are much more sparsely\\ndistributed as expected.\\nEncouraged by this, we continued to push the sparsity to an extreme form. Now for each head, we\\nonly assign a learned relative position bias for a single position. To do this, during training, we\\nmultiply the position biases wfor each layer and each head with a sample from its corresponding\\nGumbel softmax distribution [34]:\\nwi = wi ∗gumbel(wi; τ), (10)\\nwhere τ is the temperature term for Gumbel softmax, and we set it as 0.5; gumbel(wi; τ) produces a\\n(sparse) sample with the same shape as wi. During inference, the Gumbel softmax is replaced with\\nhard max, i.e., a one hot vector is returned. This results in a model with top 1 accuracy 79.9, with less\\nthan 1 point drop compared with the unregularized model. The position biases are visualized in Fig.\\n8. This extreme model variant makes it possible to implement the context reduction of K,V with a\\ncombination of global average pooling and indexing, which has the same complexity as AFT-simple\\nbut maintains strong performance (comparable to that of the standard Transformer).\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 9: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 10: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 11: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 12: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 1: Exponentiated position biases learned by AFT-full, trained on ImageNet-1K, shown from\\nlayer 1, 2, ..., 12, arranged from top left to bottom right. Each image is of size 197 × 197, where\\nthe ﬁrst element corresponds to the class token, and the remaining 196 correspond to the 14 × 14\\npositions. We see that local, sparse patterns are learned without explicit supervision.\\nTable 1: The effect of factorized parameterization\\nof AFT-full.\\nTrain loss Top 1 Acc\\nNon Factorized 3.17 78.2\\nFactorized (default) 3.08 79.8\\nTable 2: The effect of reprameterization of AFT-\\nconv (kernel size 7 × 7).\\nTrain loss Top 1 Acc\\nNaive param 3.11 79.4\\nReparameterized (default) 2.94 80.8\\n1 Additional Ablations1\\nWe conducted more experiments on the ImageNet-1K classiﬁcation settings.2\\nFactorization ofw. We ﬁrst verify the importance of the factorized parameterization of AFT-full.3\\nAs shown in Tab 1, the non factorized parameterization of AFT-full achieves worse training and test4\\nperformance than the factorized version.5\\nReparameterization ofw. For AFT-conv, we by default apply the reprameterization as described in6\\nSec. 3.2. We verify that this design effectively improves the model’s performance, as shown in Table7\\n2.8\\nKernel size.We also experimented with varying the local window size based on AFT-conv small9\\n(384 heads). The results are shown in Tab 3. Note that AFT-conv achieves comparable performance10\\nto the Deit reference even with a very small kernel size of 3 × 3.11\\nTable 3: Varying kernel size for AFT-conv.\\nKernel 3 7 11 15 25 27 DeiT small\\nTrain loss 3.02 2.94 2.94 2.93 2.93 2.94 3.01\\nTop 1 Acc 79.9 80.8 80.8 81.0 80.7 81.0 79.9\\n1\\narXiv:2105.14103v2  [cs.LG]  21 Sep 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Image completion with the AFT-local trained on CIFAR10 autoregressive modeling task.\\nTop: masked images from the test set. Bottom: completed images.\\nTable 4: Top 1 accuracy of AFT-conv without the query term (w/o q). This results in signiﬁcant\\nperformance drops.\\nKernel 11 15\\nwith q (default) 80.8 81.0\\nw/o q 79.3 79.5\\nContribution of the query.The query term contributes a small fraction to the computation of AFT,12\\nbut it contributes signiﬁcantly to AFT’s performance. We conducted an additional experiment with13\\nAFT-conv (384 heads, kernel size in11 × 11 and 15 × 15), where we remove the query term. The14\\nresult is shown in Tab 4.15\\nVisualizing the key. The keys play a central role in AFT, as they provide content dependent16\\nreweighting for effective context reduction. In order to understand their behavior, we visualized17\\nthe feature maps for a AFT-conv model on randomly sampled images from the validation set of18\\nImageNet-1K, as shown in Fig. 7, 8, 9, 10. Interestingly, we see that the keys gradually evolve to19\\n“object detectors\" as the layer level goes up.20\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 3: The full set of average relative 2d attention maps learned by a pretrained ViT model (with 12\\nlayers and 6 heads) on ImageNet-1K. Each row corresponds to a layer and each column corresponds\\nto a head. Each attention map is of size 27 × 27, with the class token excluded.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 4: Exponentiated position biases learned by AFT-conv, trained on ImageNet-1K. Each row\\ncorresponds to a layer, each column corresponds to a head (the ﬁrst 16 are shown). This model has\\ntop 1 accuracy of 80.8%.\\nFigure 5: Exponentiated position biases learned by AFT-conv (kernel size 11 × 11) with sparsity\\nregularization, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds\\nto a head (the ﬁrst 16 are shown). This model has top 1 accuracy of 80.9%.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 6: Exponentiated position biases learned AFT-conv (kernel size11×11) with Gumbel softmax\\nsampling, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a\\nhead (the ﬁrst 16 are shown). This model has top 1 accuracy of 79.9%.\\n2 Sparsity21\\nThe position biases learned by AFT-conv (kernel size11 × 11) as shown in Figure 4 demonstrates22\\ninteresting sparsity patterns, which suggests great potential for quantization and pruning. To this end,23\\nwe experimented with a simple sparsity promoting regularization term:24\\nreg(w) =\\nh∑\\ni=1\\nH(wi), H(wi) =entropy(softmax(wi)). (1)\\nWhere we simply minimize the entropy for each head, with the softmax distribution using wi as25\\nthe logits. We combining reg(w) with the cross entropy loss with a small weighting ( 0.001) and26\\ntrain with the AFT-conv with kernel size 11 and 384 heads. This results in a slight improvement27\\nin accuracy (due to its regularization effect) of 80.9 vs 80.8, as well as sparser looking position28\\nbiases. The visualization is shown in Fig. 5. We see that the position biases are much more sparsely29\\ndistributed as expected.30\\nEncouraged by this, we continued to push the sparsity to an extreme form. Now for each head, we31\\nonly assign a learned relative position bias for a single position. To do this, during training, we32\\nmultiply the position biases wfor each layer and each head with a sample from its corresponding33\\nGumbel softmax distribution [? ]:34\\nwi = wi ∗ gumbel(wi; τ), (2)\\nwhere τ is the temperature term for Gumbel softmax, and we set it as 0.5; gumbel(wi; τ) produces a35\\n(sparse) sample with the same shape as wi. During inference, the Gumbel softmax is replaced with36\\nhard max, i.e., a one hot vector is returned. This results in a model with top 1 accuracy 79.9, with less37\\nthan 1 point drop compared with the unregularized model. The position biases are visualized in Fig.38\\n6. This extreme model variant makes it possible to implement the context reduction of K,V with a39\\ncombination of global average pooling and indexing, which has the same complexity as AFT-simple40\\nbut maintains strong performance (comparable to that of the standard Transformer).41\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 7: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 8: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 9: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}, page_content='Figure 10: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/Attention is All You Need Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'Attention is All You Need Paper.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content=\"© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 197 \\n \\nMACHINE LEARNING \\n  S. Geetha Gowri1,  R.Devi 2, Dr.K.Sethuraman M.A., M.Phil., Ph.D.3 \\n1,2 Department of Computer Science, Parvathy's Arts and Science College, Dindigul, Tamilnadu. \\n3Sky (Simplified Kundalini Yoga), World Community Service Centre, Chennai 3 \\nAbstract: \\nMachine Learning is the art (and science) of enabling machin es to learn things which are not explicitly programmed. It involves as \\nmuch mathematics as much it involves computer science. Most often, people are put off by the sheer amount of mathematical equ ations \\nand concepts in machine learning. The past year has b een a great one for AI and Machine Learning. Many new high -impact \\napplications of Machine Learning were discovered and brought to light, especially in healthcare, finance, speech recognition,  \\naugmented reality, and more complex 3D and video applications.  In machine learning programming languages  they are used python to \\nSQL. In this paper we have discussed what is machine learning and its types and how does machine learning works and the key \\nelements of ML and we have also explained machine learning methods which are using today and its process, applications ,advan tages, \\ndisadvantages and top 7 programming languages and we discussed about the companies which uses machine learning.  \\n \\nKeywords: Machine learning (ML), Traditional Programming, ML algorithms. \\nINTRODUCTION: \\nMachine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and \\nimprove from experience without being explicitly programmed. Machine learning focuses on the development of computer programs  that \\ncan access data and use it learn for themselve s. Machine learning (ML) is the scientific study of algorithms and statistical models that \\ncomputer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns  and inference \\ninstead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based  on sample data, \\nknown as training data  in order to make predictions or decisions without being explicitly programmed to perform the task. Machine \\nlearning algorithms are used in a wide variety of applications, such as email filtering, and computer vision, where it is infeasible to dev elop \\nan algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistic s, which focuses \\non making predictions using computers. The study of mathematical optimization delivers methods, theory and application domain s to the \\nfield of machine learning. Data mining is a field of study within machine learning, and focuses on explora tory data analysis through \\nunsupervised learning.  Machine learning and artificial intelligence share the same definition in the minds of many however, there are some \\ndistinct differences readers should r ecognize as well. In its application across business  problems, machine learning is also referred to as \\npredictive analytics. \\n \\nLiterature Survey: \\nMachine learning is a field of computer science which gives computers an ability to learn without being explicitly programmed . \\nMachine learning is used in a variet y of computational tasks where designing and programming explicit algorithms with good performance \\nis not easy. Applications include email filtering, recognition of network intruders or malicious insiders working towards a d ata breach. One \\nof the foundatio n objectives of machine learning is to train computers to utilize data to solve a specified problem. A good number of \\napplications of machine learning like classifier training on email messages in order to differentiate between spam and non -spam messages, \\nfraud detection etc. In this article we will focus on basics of machine learning, machine learning tasks and problems and var ious machine \\nlearning algorithms. \\nMachine Learning (ML) has evolved from the endeavour of few computer enthusiasts exploiting the p ossibility of computers \\nlearning to play games, and a part of Mathematics (Statistics) that seldom considered computational approaches, to an indepen dent research \\ndiscipline that has not only provided the necessary base for statistical -computational princi ples of learning procedures, but also has \\ndeveloped various algorithms that are regularly used for text interpretation, pattern recognition, and a many other commercial purposes  and \\nhas led to a separate research interest in data mining to identify hidden regularities or irregularities in social data that growing by second. \\nThis paper focuses on explaining the concept and evolution of Machine Learning, some of the popular Machine Learning algorith ms and \\ntries to compare three most popular algorithms based on some basic notions. Sentiment140 dataset was used and performance of each \\nalgorithm in terms of training time, prediction time and accuracy of prediction have been documented and compared.  \\nThe Natural Language Processing (NLP) and machine learning  techniques used for representation of information and what \\nclassification algorithms are suitable for identifying & classifying relevant medical information in short text. This paper i s present \\nhealthcare diagnosis treatment & prevention of disease, illne ss, injury in human. The domain is automatically learn some task of healthcare \\ninformation, medical management, Patient health information etc. The proposed technique can be integrated with any medical ma nagement\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content=\"© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 198 \\n \\nsystem to make better medical decision and in patient management system can automatically mining biomedical information from digital \\nrepositories. \\nWhat is Machine Learning? \\nMachine Learning is getting computers to program themselves. If programming is automation, then machine learning is \\nautomating the process of automation.  writing software is the bottleneck, we don’t have enough good developers. Let the data do the work \\ninstead of people. Machine learning is the way to make programming scalable.  \\n\\uf0b7 Traditional Programming: Data and program is run on the computer to produce the output. \\n\\uf0b7 Machine Learning : Data and output is run on the computer to create a program. This program can be used in traditional \\nprogramming. \\nMachine learning is like farming or gardening. Seeds is the algorithms, nutrients  is the data, the gardner is you and plants is the \\nprograms. \\n \\nTraditional Programming  vs Machine Learning \\nEvolution of Machine Learning: \\nToday, machine learning is different from what it used to be in the past, due to the emergence of advanced computing \\ntechnologies. Initially, it had gained momentum due to pattern recognition and the fact that computers did not have to be pro gramed to \\nexecute certain tasks to learn. Many researchers who were interested in Artificial Intelligence (AI) investigated this ar ea further to find out \\nwhether computers could really learn from data or not. \\nThe focus here is on iterative learning. Machines begin to adapt to new data that they are exposed to, over a period. Based o n the \\npatterns and computations that are previously c reated, machines learn to repeat decisions made in the past, in similar situations. This aspect \\nof machines' ability to learn from the existing patterns, is now gaining huge momentum.  \\nToday, people are sitting up and taking notice of the fact that machines  are now able to apply complicated mathematical \\ncalculations to areas, such as big data, at a much faster rate. Consider Google Car for instance, which is primarily built on  the crux of \\nmachine learning. Another important use of machine learning can be fou nd in regular recommendations that are rolled out by companies \\nlike Netflix and Amazon - an example of machine learning in everyday life. Next, ML can also be combined with linguistic rules creation. \\nThis application is implemented by Twitter, where you wi ll know what customers say about you. And not to forget, machine learning is \\nsignificantly being used to detect fraud in various industry sectors. \\nHow Does Machine Learning Work? \\nTo get the maximum value from big data, businesses must know exactly how to p air the right algorithm with a particular tool or \\nprocess and build machine learning models based on iterative learning processes. Some of the key machines learning algorithms are  \\n\\uf0b7 Random forests \\n\\uf0b7 Neural networks \\n\\uf0b7 Discovery of sequence and associations \\n\\uf0b7 Decision trees \\n\\uf0b7 Mapping of nearest neighbor\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 199 \\n \\n\\uf0b7 Supporting vector machines \\n\\uf0b7 Boosting and bagging gradient \\n\\uf0b7 Self organizing maps \\n\\uf0b7 Multivariate adaptive regression \\n\\uf0b7 SEO \\n\\uf0b7 Analysis of principal components \\nAs mentioned above, the secret to successfully harnessing the app lications of ML lies in not just knowing the algorithms, but in pairing \\nthem accurately with the right tools and processes, which include  \\n\\uf0b7 Data exploration followed by visualization of model results \\n\\uf0b7 Overall data quality and management \\n\\uf0b7 Easy model deployment to quickly get reliable and repeatable results  \\n\\uf0b7 Developing graphical user interface for creating process flows and building models  \\n\\uf0b7 Comparing various machine learning models and identifying the best \\n\\uf0b7 Identify best performers through automated ensemble model evaluation \\n\\uf0b7 Automated data-to-decision process \\nWhy Should You Know About Machine Learning? \\n \\nThe days are gone when programmers would tell a machine how to solve a problem at hand. We are in the era of machine learning \\nwhere machines are left to solve problems, on their own, by identifying the patterns in each data set. Analyzing hidden trends and patterns \\nmakes it easy to predict future problems and prevent them from occurring.  \\nA machine learning algorithm usually follows a certain ty pe of data and then uses the patterns hidden in that data to answer more \\nquestions. For example showing a computer a series of photographs, some of which say that \"this is a horse\" and some of which  say \"this is \\nnot a horse.\" After this exercise, if you sh ow some more photographs to the same computer, it will be on a mission to identify which of \\nthose photographs are of a horse and which of those are not that of a horse. Every correct and incorrect guess of the compute r is added to its \\nmemory, which makes it smarter in the longer run and enriches its learning over a period. \\nKey Elements of Machine Learning: \\nThere are tens of thousands of machine learning algorithms and hundreds of new algorithms are developed every year.  \\nEvery machine learning algorithm has three components: \\n\\uf0b7 Representation: how to represent knowledge. Examples include decision trees, sets of rules, instances, graphical models, neural \\nnetworks, support vector machines, model ensembles and others. \\n\\uf0b7 Evaluation: the way  to evaluate candidate progr ams (hypotheses).  Examples include accuracy, prediction and recall, squared \\nerror, likelihood, posterior probability, cost, margin, entropy k-L divergence and others. \\n\\uf0b7 Optimization: the way candidate programs are generated known as  the search process. For e xample combinatorial optimization, \\nconvex optimization, constrained optimization.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 200 \\n \\nMachine Learning Methods which are using today \\nAlthough supervised and unsupervised learning are two of the most widely accepted machine learning methods by businesses \\ntoday, there are various other machine learning techniques. Following is an overview of some o f the most accepted ML methods  \\n \\n \\n \\nSupervised Learning \\nThese algorithms are trained using labeled examples, in different scenarios, as an input where the desired outcome is already \\nknown. An equipment, for instance, could have data points such as \"F\" and \"R\" where \"F\" represents \"failed\" and \"R\" represent s \"runs\". \\nA learning algorithm will receive a set of input instructions along with the corresponding accurate ou tcomes. The learning \\nalgorithm will then compare the actual outcome with the accurate outcome and flag an error, if there is any discrepancy. Usin g different \\nmethods, such as regression, classification, gradient boosting, and prediction, supervised learnin g uses different patterns to proactively \\npredict the values of a label on extra unlabeled data. This method is commonly used in areas where historical data is used to  predict events \\nthat are likely to occur in the future. For instance, anticipate when a cr edit card transaction is likely to be fraudulent or predict which \\ninsurance customers are likely to file their claims. \\nUnsupervised Learning \\nThis method of ML finds its application in areas were data has no historical labels. Here, the system will not be provided with the \\n\"right answer\" and the algorithm should identify what is being shown. The main aim here is to analyze the data and identify a  pattern and \\nstructure within the available data set. Transactional data serves as a good source of data set for unsupervised learning. \\nFor instance, this type of learning identifies customer segments with similar attributes and then lets the business to treat them \\nsimilarly in marketing campaigns. Similarly, it can also identify attributes that differentiate custome r segments from one another. Either \\nways, it is about identifying a similar structure in the available data set. Besides, these algorithms can also identify outl iers in the available \\ndata sets. \\nSome of the widely used techniques of unsupervised learning are - \\n\\uf0b7 k-means clustering \\n\\uf0b7 self-organizing maps \\n\\uf0b7 value decomposition \\n\\uf0b7 mapping of nearest neighbor'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 201 \\n \\nSemi-supervised Learning \\nThis kind of learning is used and applied to the same kind of scenarios where supervised learning is applicable. However, one  \\nmust note tha t this technique uses both unlabeled and labeled data for training. Ideally, a small set of labeled data, along with a large \\nvolume of unlabeled data is used, as it takes less time, money and efforts to acquire unlabeled data. This type of machine le arning is often \\nused with methods, such as regression, classification and prediction. Companies that usually find it challenging to meet the high costs \\nassociated with labeled training process opt for semi-supervised learning. \\nReinforcement Learning \\nThis is mai nly used in navigation, robotics and gaming. Actions that yield the best rewards are identified by algorithms that use \\ntrial and error methods. There are three major components in reinforcement learning, namely, the agent, the actions and the e nvironment. \\nThe agent in this case is the decision maker, the actions are what an agent does, and the environment is anything that an age nt interacts \\nwith. The main aim in this kind of learning is to select the actions that maximize the reward, within a specified time . By following a good \\npolicy, the agent can achieve the goal faster. \\nHence, the primary idea of reinforcement learning is to identify the best policy or the method that helps businesses in achie ving \\nthe goals faster. While humans can create a few good mode ls in a week, machine learning is capable of developing thousands of such \\nmodels in a week. \\nThe machine learning process \\nSetting up an architecture for machine learning systems and applications requires a good insight in the various processes tha t play \\na crucial role. So to develop a good architecture you should have a solid insight in:  \\n\\uf0b7 The business process in which your machine learning system or application is used.  \\n\\uf0b7 The way humans interact or act (or not) with the machine learning system. \\n\\uf0b7 The development and maintenance process needed for the machine learning system.  \\n\\uf0b7 Crucial quality aspects, e.g. security, privacy and safety aspects. \\nIn its core a machine learning process exist of a number of typical steps. These steps are:  \\n\\uf0b7 Determine the problem you want to solve using machine learning technology \\n\\uf0b7 Search and collect training data for your machine learning development process.  \\n\\uf0b7 Select a machine learning model \\n\\uf0b7 Prepare the collected data to train the machine learning model \\n\\uf0b7 Test your machine learning system using test data \\n\\uf0b7 Validate and improve the machine learning model. Most of the time you will need to search for more training data within this \\niterative loop.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 202 \\n \\nUses of Machine Learning:  \\n1. Data Security \\nMalware is a huge and growing  problem. In 2014, Kaspersky Lab said it had detected 325,000 new malware files every \\nday. But, institutional intelligence company Deep Instinct says that each piece of new malware tends to have almost the same code \\nas previous versions only between 2 and 10% of the files change from iteration to iteration. Their learning mo del has no problem \\nwith the 2 –10% variations, and can predict which files are malware with great accuracy. In other situations, machine learning \\nalgorithms can look for patterns in how data in the cloud is accessed, and report anomalies that could predict security breaches. \\n2. Personal Security  \\nIf you’ve flown on an airplane or attended a big public event lately, you almost certainly had to wait in long security \\nscreening lines. But machine learning is proving that it can be an asset to help eliminate false alarms and spot things human \\nscreeners might miss in security screenings at airports, stadiums, concerts, and other venues. That can speed up the process \\nsignificantly and ensure safer events. \\n3. Financial Trading  \\nMany people are eager to be able to predict what the stock markets will do on any given day — for obvious reasons. But \\nmachine learning algorithms are getting closer all the time. Many prestigious trading firms use proprietary systems to predic t and \\nexecute trades at high speeds and high volume. Man y of these rely on probabilities, but even a trade with a relatively low \\nprobability, at a high enough volume or speed, can turn huge profits for the firms. And humans can’t possibly compete with \\nmachines when it comes to consuming vast quantities of data or the speed with which they can execute a trade. \\n4. Healthcare  \\nMachine learning algorithms can process more information and spot more patterns than their human counterparts. One \\nstudy used computer assisted diagnosis (CAD)  when to review the early mammography scans of women who later developed \\nbreast cancer, and the computer spotted 52% of the cancers as much as a year  before the women were officially diagnosed. \\nAdditionally, machine learning can be used to understand risk factors for disease in large populations. The company Medecision \\ndeveloped an algorithm that was able to identify eight variables to predict avoidable hospitalizations in diabetes patients.  \\n5. Marketing Personalization  \\nThe more you can understand about your customers, the better you can serve them, and the more you will sell.  That’s the \\nfoundation behind marketing personalization. Perhaps you’ve had the experience in which you visit an online store and look at a \\nproduct but don’t buy it and then see digital ads across the web for that exact product for days afterward. That kind of marketing \\npersonalization is just the tip of the iceberg. Companies can personalize which emails  \\n \\na customer receives, which direct mailings or coupons, which offers they see, which products show up as “recommende d” and so \\non, all designed to lead the consumer more reliably towards a sale. \\n6. Fraud Detection  \\nMachine learning is getting better and better at spotting potential cases of fraud across many different fields. PayPal, for \\nexample, is using machine learning to fight money laundering. The company has tools that compare millions of transacti ons and \\ncan precisely distinguish between legitimate and fraudulent transactions between buyers and sellers.  \\n7. Recommendations  \\nYou’re probably familiar with this use if you use services like Amazon or Netflix. Intelligent machine learning \\nalgorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge \\nwatch next. These recommendations are getting smarter all the time, recognizing, for example, that you might purchase certain  \\nthings as gifts (and  not want the item yourself) or that there might be different family members who have different TV \\npreferences.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 203 \\n \\n8. Online Search  \\nPerhaps the most famous use of machine learning, Google and its competitors are constantly improving what the search \\nengine understands. Every time you execute a search on Google, the program watches how you respond to the results. If you click \\nthe top result and stay on that web page, we can assume you got the information you were looking for and the search was a \\nsuccess.  If, on t he other hand, you click to the second page of results, or type in a new search string without clicking any of the \\nresults, we can surmise that the search engine didn’t serve up the results  you wanted  and the program can learn from that mistake \\nto deliver a better result in the future. \\n9. Natural Language Processing (NLP)  \\nNLP is being used in all sorts of exciting applications across disciplines. Machine learning algorithms with natural \\nlanguage can stand in for customer service agents and more quickly route  customers to the information they need. It’s being used \\nto translate obscure legalese in contracts into plain language and help attorneys sort through large volumes of information t o \\nprepare for a case. \\n10. Smart Cars  \\nIBM recently surveyed top auto executives, and 74% expected that we would see smart cars on the road by 2025. A smart \\ncar would not only integrate into the Internet of Things, but also learn about its owner a nd its environment. It might adjust the \\ninternal settings — temperature, audio, seat position, etc. — automatically based on the driver, report and even fix problems itself, \\ndrive itself, and offer real time advice about traffic and road conditions. \\n \\nApplications of Machine Learning: \\n\\uf0b7 Web search: ranking page based on what you are most likely to click on. \\n\\uf0b7 Computational biology: rational design drugs in the computer based on past experiments. \\n\\uf0b7 Finance: decide who to send what credit card offers to. Evaluation of risk on credit offers. How to decide where to invest money. \\n\\uf0b7 E-commerce:  Predicting customer churn. Whether or not a transaction is fraudulent. \\n\\uf0b7 Space exploration: space probes and radio astronomy. \\n\\uf0b7 Robotics: how to handle uncertainty in new environments. Autonomous. Self-driving car. \\n\\uf0b7 Information extraction: Ask questions over databases across the web. \\n\\uf0b7 Social networks: Data on relationships and preferences. Machine learning to extract value from data.  \\n\\uf0b7 Debugging: Use in computer science problems like debugging. Labor intensive process. Could suggest where the bug could be. \\nAdvantages of Machine learning: \\n\\uf0fc Easily identifies trends and patterns \\nMachine Learning can review large volumes of data and discover specific trends and patterns that would not be apparent  \\nto humans. For instance, for an e -commerce website like Amazon, it serves to understand the browsing behaviors and purchase \\nhistories of its users to help cater to the right products, deals, and reminders relevant to them. It uses the results to rev eal relevant \\nadvertisements to them. \\n\\uf0fc No human intervention needed (automation) \\nWith ML, you don’t need to babysit your project every step of the way. Since it means giving machines the ability to \\nlearn, it lets them make predictions and also improve the algorith ms on their own. A common example of this is anti -virus \\nsoftwares; they learn to filter new threats as they are recognized. ML is also good at recognizing spam.  \\n\\uf0fc Continuous Improvement \\nAs ML algorithms  gain experience, they keep improving in accuracy and efficiency. This lets them make better \\ndecisions. Say you need to make a weather forecast model. As the amount o f data you have keeps growing, your algorithms learn \\nto make more accurate predictions faster.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 204 \\n \\n\\uf0fc Handling multi-dimensional and multi-variety data \\nMachine Learning algorithms are good at handling data that are multi -dimensional and multi-variety, and they can do this \\nin dynamic or uncertain environments. \\n\\uf0fc Wide Applications \\nYou could be an e-tailer or a healthcare provider and make ML work for you. Where it does apply, it holds the capability \\nto help deliver a much more personal experience to customers while also targeting the right customers. \\nDisadvantages of Machine Learning: \\n\\uf0fc Data Acquisition \\nMachine Learning requires massive data sets to train on, and these should be inclusive/unbiased, and of good quality. There \\ncan also be times where they must wait for new data to be generated. \\n\\uf0fc Time and Resources \\nML needs enough time to let the algorithms learn and develop enough to fulfill their purpose with a considerable amount \\nof accuracy and relevancy. It also needs massive resources to function. This can mean addition al requirements of computer power \\nfor you. \\n\\uf0fc Interpretation of Results \\nAnother major challenge is the ability to accurately interpret results generated by the algorithms. You must also carefully \\nchoose the algorithms for your purpose. \\n\\uf0fc High error-susceptibility \\nMachine Learning is autonomous but highly susceptible to errors. Suppose you train an algorithm with data sets small \\nenough to not be inclusive. You end up with biased predictions coming fro m a biased training set. This leads to irrelevant \\nadvertisements being displayed to customers. In the case of ML, such blunders can set off a chain of errors that can go undet ected \\nfor long periods of time. And when they do get noticed, it takes quite some  time to recognize the source of the issue, and even \\nlonger to correct it. \\nTop 7 programming languages that are used in Machine Learning: \\n \\n1. Python Machine Learning \\n \\nMachine learning scientists prefer Python over other languages like Java as it is better suited for tasks like sentiment analysis \\nand data mining. Python is hugely popular even among programmers – this is no secret. It is arguably the best programming language \\nat the moment. It’s not surprising then that even machine learning professionals like this language. \\nPython is extensive. And, it is easily adaptable to the tasks on hand as a general -purpose language.  Hence, it is highly \\nconvenient. App developers prefer this language especially.Developers  \\n \\n \\n \\nof Instagram and Pinterest used Python to re alize their vision. Python is accurate and  precise. It is efficient and convenient. These \\nreasons make Python a top machine learning language. \\n \\n2. Machine learning with R \\n \\nR is a graphic -based machine learning language for statistical computing. It is used in  data science for  modelling and \\nuncovering hidden patterns in voluminous data. It is a functional programming language  unlike Python which is object -oriented. Data \\nscientists at Facebook, Google, etc., use R for a variety of  purposes. It is useful in explo ring and understanding data. Moreover, it has \\nseveral machine learning algorithms. R is better than Python at data analysis and statistical tasks as it was designed specifically for this  \\npurpose.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 205 \\n \\n3. Java Machine Learning \\n \\nJava is generally used with Big Data  tools like MapR and Kafka and several data management  programs. Although \\nPython and R are more popular, Java is established and manages to stay relevant with updates. \\nThe newest version – Java 11 – is out. It potentially has enough to offer aspiring machine learning experts for them to \\nlearn it. \\n \\n4. MATLAB for Machine Learning \\nDespite being a propriety language, which requires you to purchase a license for use (Python and R  are open-source and \\nfree), Matlab has over a million users.  It is a popular numer ical computing language for professionals from engineering, \\neconomics and related backgrounds as it is rooted in mathematics. It also lays a solid foundation for you to learn  \\nPython as it is rooted in C, C++ and Java. \\n \\n5. Scala for Machine Learning \\nScala is increasingly used in data science. According to one survey, this machine learning language  has seen an increase \\nof 10% in usage every year.  This is hardly surprising as Scala is a general -purpose language, which supports functional and  \\nobject-oriented programming. It is scalable. \\nIt runs on the Java Virtual Machine (JVM) but can compete with both Java and Python. Scala is useful  in writing apps \\nand compiling web scripts. \\n \\n6. C \\nC is one of the oldest and popular languages. It is the “mother” of languages like C++, Java, Java  Script, etc. C is great \\nfor building predictive models and is a machine learning language you should consider learning. \\n \\n7. SQL \\nIt is popular among data analysts and  data scientists because it can easily compute data from NOSQL and advanced \\ndatabases. \\n \\nSQL is a good resource to have for the ETL (extract, transform, load) process.  \\n \\nThe Best Machine Learning Language \\n \\nTo sum up, Python is arguably the best programming language for machine learning, as it is a  \\ngeneral-purpose language that is suited for a variety of machine learning tasks. R is better suited for \\ndata analysis and statistical tasks as it is specifically designed for statistical computing.  \\nJava and C are popular, established machine learning programming languages that are always good  to know. Matlab is great \\nbecause it is rooted in mathematics. SQL is useful for querying large data  sets. \\nScala is another general-purpose machine learning language that potentially has a lot to offer for  \\naspiring machine learning engineers or anyone taking a machine learning course. \\n Machine Learning Using Companies: \\n1. Yelp – Image Curation at Scale  \\nFew things compare to trying out a new restaurant then going online to complain about it afterwards. This is among the many \\nreasons why Yelp is so popular (and useful). \\nWhile Yelp might not seem to be a tech company at first glance, Yelp is leveraging machine learning to improve users’ \\nexperience.this is why Yelp turned to machine learning a couple of years ago when it first implemented its picture classification \\ntechnology. Yelp’s machine learning algorithms help the company’s human staff to compile, categorize, and label images more \\nefficiently – no small feat when you’re dealing with tens of millions of photos. \\n2. Pinterest – Improved Content Discovery \\nWhether you’re a hardcore pinner or have never used the site before, Pinterest occupies a curious place in the social media \\necosystem. Since Pinterest’s primary function is to curate existing content, it makes sense that investing in technologies that c an make this \\nprocess more effective would be a priority and that’s definitely the case at Pinterest. \\nIn 2015, Pinterest acquired Kosei, a machine learning company that specialized in the commercial applications of machine learning tech \\n(specifically, content discovery and recommendation algorithms).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 206 \\n \\nToday, machine learning touches virtually every aspect of Pinterest’s business operations, from spam moderation and content  discovery to \\nadvertising monetization and reducing churn of email newsletter subscribers.  \\n3. Facebook – Chatbot Army \\n Facebook’s Messenger service is still a little contentious (people have very strong feelings about messaging apps, it seems), it’s \\none of the most exciting aspects of the world’s largest social media platform. That’s because Messenger has become something of an \\nexperimental testing laboratory for chatbots. \\n \\n \\nAny developer can create and submit a chatbot for inclusion in Facebook Messenger. This means that companies with a strong \\nemphasis on customer service and retention can leverage chatbots, even if they’re a tiny startup with limited engineering resources. \\nOf course, that’s not the only application of machine learning that Facebook is interested in. AI applications are being used at \\nFacebook to filter out spam and poor-quality content, and the company is also researching computer vision algorithms that can “read” \\nimages to visually impaired people. \\n4. Twitter – Curated Timelines \\nTwitter has been at the center of numerous controversies of late (not least of which were the much -derided decisions to round out \\neveryone’s avatars and changes to the way people are tagged in @ replies), but one of the more contentious changes we’ve seen  on Twitter \\nwas the move toward an algorithmic feed. \\nWhether you prefer to have Twitter show you “the best tweets first” (whatever that means) or as a reasonably chronological \\ntimeline, these changes are being driven by Twitter’s machine learning technology. Twitter’s AI evaluates each tweet in real time and \\n“scores” them according to various metrics. \\nUltimately, Twitter’s algorithms then display tweets that are likely to drive the most engagement. This is determined on an individual basis; \\nTwitter’s machine learning tech makes those decisions based on your individual preferences , resulting in the algorithmically curated \\nfeeds, which kinda suck if we’re being completely honest. \\n5. Google – Neural Networks and ‘Machines That Dream’ \\nThese days, it’s probably easier to list areas of scientific R&D that Google or, rather, parent company Alphabet – isn’t working \\non, rather than trying to summarize Google’s technological ambition. \\n \\nNeedless to say, Google has been very busy in recent years, having diversified into such fields as anti -aging technology, medical \\ndevices, and – perhaps most exciting for tech nerds – neural networks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 207 \\n \\nThe most visible developments in Google’s neural network research has been the DeepMind network, the “machine that \\ndreams.” It’s the same network that produced those psychedelic images everybody was talking about a while back. \\nAccording to Google, the company is researching “virtually all aspects of machine learning,” which will lead to exciting \\ndevelopments in what Google calls “classical algorithms” as well as other applications including natural language processing,  speech \\ntranslation, and search ranking and prediction systems.  \\n8. HubSpot – Smarter Sales \\nAnyone who is familiar with HubSpot probably already knows that the company has long been an early adopter of emerging \\ntechnologies, and the company proved this again earlier this month when it announced the acquisition of machine learning firm Kemvi. \\n \\n \\nPredictive lead scoring is just one of the many potential applications  of AI and machine learning. HubSpot plans to use Kemvi’s \\ntechnology in a range of applications – most notably, integrating Kemvi’s DeepGraph machine learning and natural language processing \\ntech in its internal content management system.  This, according to HubSpot’s Chief Strategy O fficer Bradford Coffey, will allow HubSpot \\nto better identify “trigger events”  – changes to a company’s structure, management, or anything else that affects day -to-day operations – \\nto allow HubSpot to more effectively pitch prospective clients and serve existing customers. \\n \\n9. IBM – Better Healthcare \\nThe inclusion of IBM might seem a little strange, given that IBM is one of the largest and oldest of the legacy technology \\ncompanies, but IBM has managed to transition from older business models to newer revenu e streams remarkably well. None of IBM’s \\nproducts demonstrate this better than its renowned AI, Watson. \\nAn example of how IBM’s Watson can be used  to test and validate self -learning behavioral models Watson may be a Jeopardy! \\nchampion, but it boasts a consi derably more impressive track record than besting human contestants in televised game shows. Watson has \\nbeen deployed in several hospitals and medical centers in recent years , where it demonstrated its aptitude for  making highly accurate \\nrecommendations in the treatment of certain types of cancers. \\nWatson also shows significant potential in the retail sector, where it could be used as an assistant to help shoppers , as well \\nas the hospitality industry. As such, IBM is now offering its Watson machine learning technology  on a license basis – one of the first \\nexamples of an AI application being packaged in such a manner.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-09-27T13:46:56+05:30', 'author': 'Students', 'moddate': '2019-09-27T13:46:56+05:30', 'source': '../data/pdf/IJRAR Report.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'IJRAR Report.pdf', 'file_type': 'pdf'}, page_content='© 2019 IJRAR June 2019, Volume 6, Issue 2                                           www.ijrar.org  (E-ISSN 2348-1269, P- ISSN 2349-5138) \\nIJRAR1ARP035 International Journal of Research and Analytical Reviews (IJRAR) www.ijrar.org 208 \\n \\nConclusion: \\n \\nMachine Learning is a technique of training machines to perform the activities a human brain can do, albeit bit faster and better \\nthan an average human-being. Today we have seen that the machines can beat human champions in games such as Chess, AlphaGO, which \\nare considered very complex. Yo u have seen that machines can be trained to perform human activities in several areas and can aid humans \\nin living better lives. Machine  Learning can be a Supervised or Unsupervised. If you have lesser amount of data and clearly labelled data \\nfor training, opt for Supervised Learning. Unsupervised Learning would generally give better performance and results for large data sets. I f \\nyou have a huge data set easily available, go for deep learning techniques. You also have learned Reinforcement Learning and Deep \\nReinforcement Learning. You now know what Neural Networks are, their applications and limitations.  \\n \\nREFERENCES: \\n \\n1. Diksha Sharma Neeraj Kumar A Review on Machine Learning Algorithms, Tasks and Applications, International Journal of \\nAdvanced Research in Computer Engineering & Technology (IJARCET) Volume 6, Issue 10, October 2017,  ISSN: 2278  – 1323 \\n2. SUPERVISED MACHINE LEARNING APPROACHES: A SURVEY Iqbal Muhammad1 and Zhu Yan2 School of Information \\nSciences and Technology, Southwest Jiaotong University, China. DOI: 10.21917/ijsc.2015.0133  \\n3. James Cussens, “Machine Learning”, IEEE Journal of Computing and Control, Vol. 7, No. 4, pp 164 -168, 1996. \\n4. D. Aha, “Lazy Learning”, Dordrecht: Kluwer Academic Publishers, 1997.A. Coats and B. Huval, “Deep Learning with COTS \\nHPS systems”, Journal of Machine Learning Research, Vol. 28, No. 3, pp. 1337 -1345, 2013. \\n5. Steven L. Salzberg, “Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Inc., 1993”, Machine Learning, \\nVol. 16, No. 3, pp. 235-240, 1994. \\n6. S. Tong, D. Koller, \"Support vector machine active learning with applications to text classification\",  J. Mach. Learn. Res., vol. 2, \\npp. 45-66, Nov. 2002. \\n7. Pravin Shinde Kharghar Navi Mumbai. Prof.Sanjay Jadhav Health Analysis System Using Machine Learning Saraswati College \\nOf Engineering, Kharghar Navi Mumbai (IJCSIT) International Journal of Computer Science and Information Technologies, \\nVol.5(3),2014, 3928-3933 www.ijcsit.com. \\n8. A Survey on Machine Learning: Concept, Algorithms and Applications Kajaree Das1 , Rabi Narayan Behera 2 International \\nJournal of Innovative Research in Computer and Communication Engineering (An ISO 3297: 2007 Certified Organization) \\nWebsite: www.ijircce.com Vol. 5, Issue 2, February 2017 Copyright to IJIRCCE DOI: 10.15680/IJIRCCE.2017. 0502001 1301')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fa6c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=500,chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "   \n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "    print (f\"Metadata: {split_docs[0].metadata}\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f884066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 52 documents into 301 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: An Attention Free Transformer\n",
      "Shuangfei Zhai\n",
      "Apple Inc.\n",
      "szhai@apple.com\n",
      "Walter Talbott\n",
      "Apple Inc.\n",
      "wtalbott@apple.com\n",
      "Nitish Srivastava\n",
      "Apple Inc.\n",
      "nitish_srivastava@apple.com\n",
      "Chen Huang\n",
      "Apple Inc.\n",
      "chen...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-09-23T00:04:06+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2021-09-23T00:04:06+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/Attention Free Transformer.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': 'Attention Free Transformer.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavyananda/rag_proj/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import numpy as np \n",
    "import chromadb.config as settings \n",
    "import uuid\n",
    "from typing import List,Dict,Any,tuple \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_proj (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
